{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGCNYnIyOgxkXp7LjxdaSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saksham-chand/Brain_Tumor_Detection_Using_DL/blob/main/Brain_Tumor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brain Tumor Detection Using DL"
      ],
      "metadata": {
        "id": "ysbzzMScvPzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "zipfile module is used to provide tool for working on zip file\n",
        "\n",
        "with is a keyword to open the zipfile in r (read) mode\n",
        "\n",
        "\"as zip\" assigns the opened Zip file object to zip variable\n",
        "extract all extracts all the contents of zip file"
      ],
      "metadata": {
        "id": "RP7YnxSYvbap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "dataset = \"/content/Brain_Tumor_Dataset.zip\"\n",
        "with ZipFile(dataset, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print(\"Done\")"
      ],
      "metadata": {
        "id": "sd4a6sg_Qc0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b834f6-26d7-4a95-e5b5-6083ddd2d8fd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy is a module with the help of which we can perform all the array operations\n",
        "\n",
        "Matplotlib.pyplot is used for 2D plotting and vizualizing data\n",
        "\n",
        "OS allows use functions related to working directories\n",
        "\n",
        "Shutil helps use to copy and remove files from or to a folder"
      ],
      "metadata": {
        "id": "AUMsW4JavlPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import shutil"
      ],
      "metadata": {
        "id": "JJusQdg9pQYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROOT_DIR is used to set the path of of our dataset\n",
        "\n",
        "images is a dictionary used to store the counts of Tumor and Healthy images\n",
        "\n",
        "os.listdir(ROOT_DIR) returns a list of all the subdirectories present in our dataset for example here Tumor and Healthy are subdirectories\n",
        "\n",
        "os.path.join is used to construct a full path from the dataset to the subdirectories\n",
        "\n",
        "len function counts the number of images and stores them corresponding to respective KEY value ie dir/subdirectories"
      ],
      "metadata": {
        "id": "aEtnZhgevo8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='/content/Brain_Tumor_Dataset'\n",
        "images={}\n",
        "for dir in os.listdir(ROOT_DIR):\n",
        "    images[dir]=len(os.listdir(os.path.join(ROOT_DIR,dir)))"
      ],
      "metadata": {
        "id": "zgq4y7eWpc8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KwDp_Hhp1p_",
        "outputId": "7d02aa2d-fb87-4f19-e955-0dcfac6a52bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('Healthy', 1950), ('Tumor', 3516)])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions of os.listdir()"
      ],
      "metadata": {
        "id": "f0eqh4OgvwVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/Brain_Tumor_Dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC06_civqOJa",
        "outputId": "eadc28b9-e944-40ab-d9f7-24d4c8774083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Healthy', 'Tumor']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check if the train and test directory already exists\n",
        "\n",
        "if not create both the directories using mkdir (make directories)\n",
        "\n",
        "for each directory (test and train) make a subdirectories corresponding to the initial subdirectories(dir) ie HEALTHY and TUMOR\n",
        "\n",
        "Now randomly choose images from the Subdirectories 1st from Healthy and then from Tumor and os.listdir(full_path) tells us from where we need to pick the data\n",
        "\n",
        "now take 70% of the total images in a subdirectory, we did -5 so that not all the images might get moved to test/train datasets and replace is FALSE it ensures that each image is selected only once\n",
        "\n",
        "Now O is a varibale which hold full path to a particular image and D is a variable which holds full path to our destionation directory\n",
        "\n",
        "By using shutil.copy we copy the image from O to D"
      ],
      "metadata": {
        "id": "4ZVvCSh0v1Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"./train\"):\n",
        "  os.mkdir(\"./train\")\n",
        "  for dir in os.listdir(ROOT_DIR):\n",
        "    os.makedirs(\"./train/\"+dir)\n",
        "    for img in np.random.choice(a=os.listdir(os.path.join(ROOT_DIR,dir)),size= (math.floor(70/100*images[dir])-5),replace=False):\n",
        "      O=os.path.join(ROOT_DIR,dir,img)\n",
        "      D=os.path.join(\"./train\",dir, img)  # Also add the image name to the destination path\n",
        "      shutil.copy(O, D)  # Copy instead of removing\n",
        "else:\n",
        "  print(\"already exist\")\n",
        "\n",
        "if not os.path.exists(\"./test\"):\n",
        "  os.mkdir(\"./test\")\n",
        "  for dir in os.listdir(ROOT_DIR):\n",
        "    os.makedirs(\"./test/\"+dir)\n",
        "    for img in np.random.choice(a=os.listdir(os.path.join(ROOT_DIR,dir)),size= (math.floor(30/100*images[dir])-5),replace=False):\n",
        "      O=os.path.join(ROOT_DIR,dir,img)\n",
        "      D=os.path.join(\"./test\",dir, img)  # Also add the image name to the destination path\n",
        "      shutil.copy(O, D)  # Copy instead of removing\n",
        "else:\n",
        "  print(\"already exist\")"
      ],
      "metadata": {
        "id": "Pwt8fmfm1XxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images={}\n",
        "for dir in os.listdir(ROOT_DIR):\n",
        "    images[dir]=len(os.listdir(os.path.join(ROOT_DIR,dir)))\n",
        "images.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPu1__NwtbXV",
        "outputId": "e286d56b-9915-4ddd-d18e-bf5f3c257eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('Healthy', 1950), ('Tumor', 3516)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have inputed several Layer from Keras\n",
        "\n",
        "CONV2D used processing and detecting/extracting features in 2-dimensional data, such as images\n",
        "\n",
        "MaxPool2D is used to reduce the size of an image by dividing the image into a Matrix and then selecting the max values\n",
        "\n",
        "Dropout is used to avoid overfiting\n",
        "\n",
        "Flatten to convert 2D image to 1D array\n",
        "\n",
        "Dense Layer where each input node is connected to each output node\n",
        "\n",
        "BatchNormalization is used to stabalize the inputs of a neural and train them\n",
        "\n",
        "GlobalAvgPool2D Layer is computes the avg of all the feautures and reduces dimensions to a signle valur per feature\n",
        "\n",
        "Sequential starts building a stack of layers\n",
        "\n",
        " Adam is an optimization algorithm used to train deep learning models\n",
        "\n",
        " ImageDataGenerator used to transform and process the image through dimensions and color"
      ],
      "metadata": {
        "id": "uir-KlQtv7Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization,GlobalAvgPool2D\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "_zKuHX36tjoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32 filter each of size 3x3 , activation function relu introduces non linearity and we have an input shape of 224*224 with three colors RGB\n",
        "\n",
        "Pooling window size is 2x2 ie it is used to select the maximum value from a matric of 2*2 size\n",
        "\n",
        "Dropout layer randomly ignore 50% of neurons in previous layer and make its output as 0 to avoid overfitting for each epoch\n",
        "\n",
        "Flatten converts 2D image to 1D vector and pass it to Dense Layer or a Multilayer Perceptrons\n",
        "\n",
        "128 nuerons in each layer which accepts and send outputs and activation function is ReLu\n",
        "\n",
        "Last layer ie the output layer has 1 nueron\n",
        "\n",
        "Lastly we compile our model with adam as optimizer (optimizer basically adjusts the weight of nueral networks to reduce loss function)\n",
        "\n",
        "loss function used for Binary Classification it tells how much the values deviate between 0 and 1\n",
        "and parameters on which the model will measure its perfromance is accuracy"
      ],
      "metadata": {
        "id": "M3DGVQbSwAwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPool2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPool2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPool2D((2, 2)),\n",
        "    Dropout(0.5),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "L_fMcYyluRmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c32fca8-4957-42ff-e75c-2d4b0022e8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 26, 26, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 86528)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               11075712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11169089 (42.61 MB)\n",
            "Trainable params: 11169089 (42.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA AUGMENTATION**\n",
        "\n",
        "Augments and prepares images for training by applying transformations like rescaling pixel values to make them in range from 0 to 1, shearing/rotating randomly, zooming at random points, and flipping horizontaly to generate new data\n",
        "\n",
        "Data Augmentation is basically a technique in which we train a model with huge amount of data by transforming the original data only to avoid overfitting\n",
        "\n",
        "flow from directory tells us the location of the image, target size basically resizes all the image before feeding them to the model , no of images transfered to the model at a time and binary classification"
      ],
      "metadata": {
        "id": "4LNrSumhwEmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
        "train_data=train_images.flow_from_directory(directory='/content/train',target_size=(224,224),batch_size=32,class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJLG57TnwgSI",
        "outputId": "e5c96962-f27a-4ab3-d55f-37d927ee35e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3816 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images=ImageDataGenerator(rescale=1./255)\n",
        "test_data=test_images.flow_from_directory(directory='/content/test',target_size=(224,224),batch_size=32,class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZgINr6px-uq",
        "outputId": "4dbc230a-a99a-4dbd-bd4e-84c8c24833b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1629 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping is used to stop training when a monitored metric (here accuracy) has stopped improving. This helps prevent overfitting val_accuracy is used to monitor the perfromance of the model, pateince waits for the number of epochs till no improvement\n",
        "\n",
        "Verbose is used to display the result of training HDF5 is most common file format for storing large data\n",
        "\n",
        "ModelCheckPoint saves the model after each epoch and saves the best model only"
      ],
      "metadata": {
        "id": "CxHrn-m5wHHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "es=EarlyStopping(monitor=\"val_accuracy\",patience=5,verbose=1,mode='max')\n",
        "mc=ModelCheckpoint(monitor=\"val_accuracy\",filepath=\"./bestmodel.h5\",verbose=1,save_best_only=True,mode='max')\n",
        "cb=[es,mc]\n"
      ],
      "metadata": {
        "id": "OCedODV314a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use fit method to train out model which trains on train_data and validates the model using unseen/new data (test data) ,epochs =30 means how many times the model will iterate over the dataset (how many times will be training data pass through our model) and callbacks is used to save the best model\n",
        "\n",
        "steps_per_epoch=len(train_data) specifies how many batches of data the model should process before completing one epoch\n",
        "\n",
        "validation_steps=len(test_data) specifies how many batches of validation data (test_data) the model should process at the end of each epoch.\n",
        "here one batch has 32 images"
      ],
      "metadata": {
        "id": "Edw1gkJtwMKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hs = model.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    epochs=30,\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data),\n",
        "    callbacks=cb\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26HiXuQm2ut2",
        "outputId": "e83bedaf-c427-45c3-8967-a5039dadd686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8459\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94966, saving model to ./bestmodel.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/120 [==============================] - 520s 4s/step - loss: 0.3658 - accuracy: 0.8459 - val_loss: 0.1496 - val_accuracy: 0.9497\n",
            "Epoch 2/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9313\n",
            "Epoch 2: val_accuracy improved from 0.94966 to 0.96440, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 516s 4s/step - loss: 0.1896 - accuracy: 0.9313 - val_loss: 0.1148 - val_accuracy: 0.9644\n",
            "Epoch 3/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9358\n",
            "Epoch 3: val_accuracy did not improve from 0.96440\n",
            "120/120 [==============================] - 518s 4s/step - loss: 0.1766 - accuracy: 0.9358 - val_loss: 0.1174 - val_accuracy: 0.9595\n",
            "Epoch 4/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9481\n",
            "Epoch 4: val_accuracy did not improve from 0.96440\n",
            "120/120 [==============================] - 512s 4s/step - loss: 0.1470 - accuracy: 0.9481 - val_loss: 0.1343 - val_accuracy: 0.9484\n",
            "Epoch 5/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9458\n",
            "Epoch 5: val_accuracy did not improve from 0.96440\n",
            "120/120 [==============================] - 513s 4s/step - loss: 0.1411 - accuracy: 0.9458 - val_loss: 0.1247 - val_accuracy: 0.9540\n",
            "Epoch 6/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9526\n",
            "Epoch 6: val_accuracy did not improve from 0.96440\n",
            "120/120 [==============================] - 509s 4s/step - loss: 0.1321 - accuracy: 0.9526 - val_loss: 0.0877 - val_accuracy: 0.9644\n",
            "Epoch 7/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9555\n",
            "Epoch 7: val_accuracy improved from 0.96440 to 0.97483, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 521s 4s/step - loss: 0.1224 - accuracy: 0.9555 - val_loss: 0.0724 - val_accuracy: 0.9748\n",
            "Epoch 8/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9607\n",
            "Epoch 8: val_accuracy improved from 0.97483 to 0.97667, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 530s 4s/step - loss: 0.1114 - accuracy: 0.9607 - val_loss: 0.0789 - val_accuracy: 0.9767\n",
            "Epoch 9/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9599\n",
            "Epoch 9: val_accuracy did not improve from 0.97667\n",
            "120/120 [==============================] - 510s 4s/step - loss: 0.1074 - accuracy: 0.9599 - val_loss: 0.0633 - val_accuracy: 0.9754\n",
            "Epoch 10/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9631\n",
            "Epoch 10: val_accuracy improved from 0.97667 to 0.98097, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 504s 4s/step - loss: 0.0998 - accuracy: 0.9631 - val_loss: 0.0545 - val_accuracy: 0.9810\n",
            "Epoch 11/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9665\n",
            "Epoch 11: val_accuracy did not improve from 0.98097\n",
            "120/120 [==============================] - 515s 4s/step - loss: 0.0932 - accuracy: 0.9665 - val_loss: 0.0658 - val_accuracy: 0.9797\n",
            "Epoch 12/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9696\n",
            "Epoch 12: val_accuracy did not improve from 0.98097\n",
            "120/120 [==============================] - 515s 4s/step - loss: 0.0825 - accuracy: 0.9696 - val_loss: 0.0483 - val_accuracy: 0.9810\n",
            "Epoch 13/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9701\n",
            "Epoch 13: val_accuracy did not improve from 0.98097\n",
            "120/120 [==============================] - 510s 4s/step - loss: 0.0749 - accuracy: 0.9701 - val_loss: 0.0774 - val_accuracy: 0.9748\n",
            "Epoch 14/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9706\n",
            "Epoch 14: val_accuracy improved from 0.98097 to 0.98158, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 508s 4s/step - loss: 0.0809 - accuracy: 0.9706 - val_loss: 0.0477 - val_accuracy: 0.9816\n",
            "Epoch 15/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9767\n",
            "Epoch 15: val_accuracy did not improve from 0.98158\n",
            "120/120 [==============================] - 506s 4s/step - loss: 0.0732 - accuracy: 0.9767 - val_loss: 0.0808 - val_accuracy: 0.9699\n",
            "Epoch 16/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9730\n",
            "Epoch 16: val_accuracy improved from 0.98158 to 0.98527, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 540s 5s/step - loss: 0.0747 - accuracy: 0.9730 - val_loss: 0.0451 - val_accuracy: 0.9853\n",
            "Epoch 17/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9756\n",
            "Epoch 17: val_accuracy improved from 0.98527 to 0.98772, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 506s 4s/step - loss: 0.0682 - accuracy: 0.9756 - val_loss: 0.0353 - val_accuracy: 0.9877\n",
            "Epoch 18/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9798\n",
            "Epoch 18: val_accuracy did not improve from 0.98772\n",
            "120/120 [==============================] - 504s 4s/step - loss: 0.0605 - accuracy: 0.9798 - val_loss: 0.0470 - val_accuracy: 0.9877\n",
            "Epoch 19/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9806\n",
            "Epoch 19: val_accuracy did not improve from 0.98772\n",
            "120/120 [==============================] - 505s 4s/step - loss: 0.0558 - accuracy: 0.9806 - val_loss: 0.0690 - val_accuracy: 0.9804\n",
            "Epoch 20/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9793\n",
            "Epoch 20: val_accuracy improved from 0.98772 to 0.99018, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 499s 4s/step - loss: 0.0549 - accuracy: 0.9793 - val_loss: 0.0230 - val_accuracy: 0.9902\n",
            "Epoch 21/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9811\n",
            "Epoch 21: val_accuracy did not improve from 0.99018\n",
            "120/120 [==============================] - 496s 4s/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.0651 - val_accuracy: 0.9779\n",
            "Epoch 22/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9796\n",
            "Epoch 22: val_accuracy did not improve from 0.99018\n",
            "120/120 [==============================] - 500s 4s/step - loss: 0.0629 - accuracy: 0.9796 - val_loss: 0.0518 - val_accuracy: 0.9859\n",
            "Epoch 23/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9743\n",
            "Epoch 23: val_accuracy improved from 0.99018 to 0.99263, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 507s 4s/step - loss: 0.0661 - accuracy: 0.9743 - val_loss: 0.0269 - val_accuracy: 0.9926\n",
            "Epoch 24/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9858\n",
            "Epoch 24: val_accuracy improved from 0.99263 to 0.99325, saving model to ./bestmodel.h5\n",
            "120/120 [==============================] - 507s 4s/step - loss: 0.0426 - accuracy: 0.9858 - val_loss: 0.0247 - val_accuracy: 0.9932\n",
            "Epoch 25/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9835\n",
            "Epoch 25: val_accuracy did not improve from 0.99325\n",
            "120/120 [==============================] - 507s 4s/step - loss: 0.0451 - accuracy: 0.9835 - val_loss: 0.0237 - val_accuracy: 0.9914\n",
            "Epoch 26/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9838\n",
            "Epoch 26: val_accuracy did not improve from 0.99325\n",
            "120/120 [==============================] - 508s 4s/step - loss: 0.0431 - accuracy: 0.9838 - val_loss: 0.0424 - val_accuracy: 0.9853\n",
            "Epoch 27/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9840\n",
            "Epoch 27: val_accuracy did not improve from 0.99325\n",
            "120/120 [==============================] - 501s 4s/step - loss: 0.0398 - accuracy: 0.9840 - val_loss: 0.0199 - val_accuracy: 0.9914\n",
            "Epoch 28/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9853\n",
            "Epoch 28: val_accuracy did not improve from 0.99325\n",
            "120/120 [==============================] - 534s 4s/step - loss: 0.0405 - accuracy: 0.9853 - val_loss: 0.0220 - val_accuracy: 0.9926\n",
            "Epoch 29/30\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9851\n",
            "Epoch 29: val_accuracy did not improve from 0.99325\n",
            "120/120 [==============================] - 517s 4s/step - loss: 0.0391 - accuracy: 0.9851 - val_loss: 0.0243 - val_accuracy: 0.9902\n",
            "Epoch 29: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.evaluate basically returns a list of loss function and accuracy"
      ],
      "metadata": {
        "id": "KB2-ssU1hXPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accu=model.evaluate(test_data)[1]\n",
        "print(accu)"
      ],
      "metadata": {
        "id": "w7_dOJbEGLDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4538ce8a-628e-4d83-b3e1-ff8ddc59d883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 54s 1s/step - loss: 0.0243 - accuracy: 0.9902\n",
            "0.9901780486106873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model=load_model(\"./bestmodel.h5\")"
      ],
      "metadata": {
        "id": "M-AzxtAXG0x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have loaded the path and the target size of the image then we have converted it to numpy array\n",
        "\n",
        "we have normalized the pixel range"
      ],
      "metadata": {
        "id": "-rgWRPNJhnUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "path=\"/content/Tumor2.jpg\"\n",
        "image=load_img(path,target_size=(224,224))\n",
        "image=img_to_array(image)\n",
        "image=np.expand_dims(image,axis=0)\n",
        "image=image/255\n",
        "pred=model.predict(image)[0][0]\n",
        "if pred > 0.5:\n",
        "  print(\"Tumor\")\n",
        "else:\n",
        "  print(\"Healthy\")\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "YcZr9QhTHB5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}